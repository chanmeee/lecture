---
title: "Chapter 1 Homework"
author: "1678226 Chanmi  Yoo ("
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Regression Analysis  
20657-02  
Jae Keun Yoo  
Sep 23, 2019  


### Q1.2
##### Let's denote Y = yearly cost for memership, X = the number of visits during the year.
##### Then, the relation between X and Y is:
##### Y = 300 + 2X
##### Since every observation is on the line, it is functional relation.

### Q1.7
### (a)
##### The regression model would be Y = 20X + 100, so the point estimate for X=5 is:

```{r}
100 + 20*5
```
##### However, in order to state the probability, we should know the form of distribution of the error term (and hence of the $Y_i$). So, with these limited information, we cannot solve (a) question.

### (b)
##### In (b) question, the normality assumption for the error terms is given. $Y_i$ follows the normal distribution with mean of $E(Y_{i}) = \beta_0 + \beta_1 X_i$ and variance of $\sigma^2$. Thus, we can now state the exact probability. 
##### When X=5, there is an observed value of Y, so $E(Y) = 100 + 100 = 200$ and $sigma^2 = 25$. Therefore, 
$$Y \text {~ } N(200, 25)$$ 
$P(195 < Y < 205) = P(\frac{195-200}{5} < Z < \frac{205-200}{5}) = P(-1<Z<1)$

```{r}
2*(pnorm(1) -.5)
```
##### Thus, the probability would be 0.6826.  

### Q1.10
##### No.
##### 회귀모형은 필연적으로 X가 Y의 원인임을 보여주지 않으며, 동일한 X에 대해서 실제 Y의 관측값은 다를 수 있다. 그리고 회귀모형을 구축할 때 제한된 영역을 초과한 범위에 대한 통계적 관계의 근거를 제시하지 않는다. 따라서 영역을 크게 벗어나는 범위에서 여전히 회귀식이 유효할지 보장되지 않는다. 따라서 47까지 증가하다가 감소하는 curlinear regression에 대하여 "47까지 증가하다가 감소한다"라확 단정지을 수 없다. 

### Q1.18
##### False. 
$e_{i}=Y_i - \hat{Y}_i$로 관측된 값 $Y_i$와 그에 대응하는 적합값 $\hat{Y}$ 사이의 차이이다.  
$\varepsilon_i$는 추정한 것이 아닌 참 회귀선으로부터 $Y_i$까지의 수직편차를 말하며, 알 수 없는 값이므로 $\sum{\varepsilon_i}=0$은 사실이 아니다.

### Q1.21
### (a)

```{r}
CH0121 <- read.table("../Data Sets/Chapter  1 Data Sets/CH01PR21.txt"); names(CH0121) <- c("Y", "X"); attach(CH0121)

x.bar <- mean(X)
y.bar <- mean(Y)
sxy <- sum((X-x.bar)*(Y-y.bar))
sxx <- sum((X-x.bar)^2)
b1 <- sxy/sxx; print(b1)
b0 <- y.bar - b1*x.bar; print(b0)
```

##### Thus, the estimated regression function is Y = 10.2 + 4.0*X.

##### The plot of the estimated regression function is like this:
```{r}
plot(X, Y) # scatter plot
abline(b0, b1) # adding the regression line
```

##### The linear regression function appears to give a good fit, since the regression line has little distance from the scattering of points. The regression line explains the relationship between the number of aircraft-shipment transfer (X) and the number of broken amplues (Y) well.


### (b)
##### The point estimate of the expected number of broken ampules when X = 1 is:
```{r}
10.2 + 4.0*1
```

### (c)
##### The slope is:
```{r}
b1
```
##### Therefore, broken ampules are expected to increase by 4, as the transfer increases by 1.

### (d)
```{r}
10.2 + 4.0*x.bar == y.bar
```
##### Since y.bar equals to the estimated regression function of x.bar, the point (x.bar, y.bar) goes through the fitted regression line. 

### Q1.25
### (a)
```{r}
fitted <- b0 + b1*X
resid <- Y - fitted
resid[1]  # the residual of the first case
```

##### The difference between error and residual is that error is yield from the true regression line (which is unknown) but residuals are calculated from the estimated line (which is made from observed values). So we do not know errors, but know residuals. As the estimated line goes close to the true regression line, the residual becomes better estimate of the error.

### (b)
#####  is:
```{r}
SSE <- sum(resid^2); print(SSE)  # the sum of squared residuals
MSE <- SSE / (nrow(CH0121) - 2); print(MSE)  # MSE
```

##### The sqaure root of MSE is the point estimate of sigma, the standard deviation of the probability distribution of Y for any X. In this case, 1.48 transfers is the standard deviation of this distribution.
